{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About TensorFlow and Deep Learning:-\n",
    "\n",
    "**TensorFlow:** A Python library that is used for modelling/implementation of deep learning models/networks. TensorFlow = Tensors + Flow. Tensor corresponds to the way data is represented in this library and Flow depicts the flow of these Tensors through the <i>computation graph</i>.\n",
    "\n",
    "A <i>computation graph</i> is series of TensorFlow operations arranged into a graph of nodes.\n",
    "\n",
    "**Tensors**: The standard way of representing data in TensorFlow. Tensors are nothing but multidimensional arrays, an extension of matrices (2D tables) to data with higher dimensions.\n",
    "\n",
    "Note: Only tensors may be passed between nodes in a computation graph.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li> Dimensionality is measured as **Ranks**:-\n",
    "\n",
    "<ol>\n",
    "    \n",
    "   <li> Rank 0 -> a Scalar, example: s = 482\n",
    "\n",
    "   <li> Rank 1 -> a Vector, example: v = [1, 2, 3]\n",
    "\n",
    "   <li> Rank 2 -> a Matrix, example: m = [[1,5,6], [2,5,6], [3,5,6]]\n",
    "\n",
    "   <li> Rank 3 -> a 3-Tesor or a cube holding some data, example: t = [[[1,5,6], [2,5,6], [3,5,6]], [[1,5,6], [2,5,6], [3,5,6]], [[1,5,6], [2,5,6], [3,5,6]]]\n",
    "    \n",
    "</ol>\n",
    "\n",
    "    ...\n",
    "\n",
    "   ** Rank n -> n-Tensor **\n",
    "\n",
    "<li> Tensor Data Types:- \n",
    "\n",
    "Tensorflow automattically assigns the correct data type. If you want to specifically assign the data type in order to save memory or do some other operation, it is possible.\n",
    "\n",
    "<ol>\n",
    "  \n",
    "   <li> DT_FLOAT -> tf.float32\n",
    "    \n",
    "   <li> DT_DOUBLE-> tf.float64\n",
    "    \n",
    "   <li> DT_INT8 -> tf.int8\n",
    "\n",
    "   <li> DT_UINT8 -> tf.uint8\n",
    "    \n",
    "   ...\n",
    "    \n",
    "   <li> DT_INT64 -> tf.int64\n",
    "    \n",
    "   <li> DT_STRING -> tf.string\n",
    "    \n",
    "   <li> DT_BOOL -> tf.bool\n",
    "    \n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Coding structure**\n",
    "\n",
    "It consists of two sections in particular:-\n",
    "\n",
    "<ul>\n",
    "    <li> Building a computation graph.\n",
    "    <li> Running the computation graph.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** - Building a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_10:0\", shape=(), dtype=float32) Tensor(\"Const_11:0\", shape=(), dtype=float32) Tensor(\"mul_5:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Import the TensorFlow library\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Defining two constant nodes\n",
    "aNode = tf.constant(5.0)\n",
    "bNode = tf.constant(1.9, tf.float32)\n",
    "\n",
    "cNode = aNode*bNode;\n",
    "\n",
    "# At this point, they are just abstract Tensors and not actual calculations are running.\n",
    "# Only operations are created.\n",
    "print (aNode, bNode, cNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** - Running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 1.9]\n",
      "9.5\n"
     ]
    }
   ],
   "source": [
    "# To execute the graph, we run it inside a session\n",
    "# A session places graph operations onto devices such as CPU/GPU.\n",
    "\n",
    "''' Method 1 '''\n",
    "\n",
    "aSession = tf.Session()\n",
    "\n",
    "print (aSession.run([aNode, bNode]))\n",
    "# You need to close a session in order to free up the resources it used.\n",
    "aSession.close()\n",
    "\n",
    "''' Method 2 '''\n",
    "\n",
    "with tf.Session() as bSession:\n",
    "    # You just need to run the output Tensor\n",
    "    output = bSession.run(cNode)\n",
    "    print (output)\n",
    "bSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing TensorFlow: TensorBoard**\n",
    "\n",
    "TensorBoard is a suite of web-applications for understanding your Tensor graphs.\n",
    "\n",
    "The most convinient way to do this is using FileWriter by:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aNode = tf.constant(5.0)\n",
    "bNode = tf.constant(1.9, tf.float32)\n",
    "cNode = aNode*bNode;\n",
    "\n",
    "with tf.Session() as bSession:\n",
    "    output = bSession.run(cNode)    \n",
    "    # Give the first argument as a path and the second as the graph of the session\n",
    "    aFileWriter = tf.summary.FileWriter('aSimpleGraph', aSession.graph)\n",
    "bSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the log, go to the directory in your command line and type:\n",
    "\n",
    "   ** tensorboad --logdir=\"TensorFlow\" **\n",
    "   \n",
    "It will say that the TensorBoard runs at http://localhost:6006/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been seeing constant nodes in the previous examples. Let us dig deeper into them and other nodes in TensorFlow\n",
    "\n",
    "**1. Constants**: As the name suggests, they have hardcoded values and take in no inputs. All they do is output this internal value in an active session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Placeholders**: These nodes are able to take in external input, assuming that they will be provided one at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  [[0.87102941 0.8044195 ]] \n",
      "B:  [[0.43596265 0.18756064]] \n",
      "A+B:  [[1.306992  0.9919802]]\n",
      "\n",
      "X: [[0.68873025 0.70586041 0.33634668 ... 0.89002756 0.53578776 0.40702574]\n",
      " [0.92456983 0.99341682 0.04845532 ... 0.27990118 0.9743113  0.7921947 ]\n",
      " [0.93033919 0.31557995 0.52875053 ... 0.54725981 0.11413419 0.80530257]\n",
      " ...\n",
      " [0.17897704 0.20638099 0.93131107 ... 0.37932413 0.26623807 0.61323282]\n",
      " [0.70097532 0.8168579  0.18886742 ... 0.43863398 0.65556969 0.81344977]\n",
      " [0.63265126 0.22231365 0.24139989 ... 0.69689983 0.68965193 0.70598646]] \n",
      "X*X: [[268.01447 255.37022 265.48386 ... 260.77008 256.93347 264.95584]\n",
      " [262.37158 248.21074 258.25925 ... 252.96368 250.4609  254.59473]\n",
      " [263.54474 246.0625  256.24838 ... 257.88358 254.6499  256.80545]\n",
      " ...\n",
      " [266.49362 254.3628  265.47583 ... 261.80026 258.68918 264.70462]\n",
      " [257.20786 241.20898 254.83875 ... 248.1103  254.61093 255.64911]\n",
      " [256.97766 241.38652 251.75496 ... 246.01385 243.98114 249.55927]]\n"
     ]
    }
   ],
   "source": [
    "aNode = tf.placeholder(tf.float32)\n",
    "bNode = tf.placeholder(tf.float32)\n",
    "\n",
    "summedNode = aNode + bNode;\n",
    "# You can also use the in-build TensorFlow operations\n",
    "# summedNode = tf.add(aNode, bNode);\n",
    "\n",
    "xNode = tf.placeholder(tf.float32, shape=(1024, 1024))\n",
    "# matmul is the in-built TensorFlow operation for matrix-multiplication\n",
    "yNode = tf.matmul(xNode, xNode)\n",
    "\n",
    "with tf.Session() as aSession:\n",
    "    # ERROR: will fail because placeholder was not fed.\n",
    "    #output = aSession.run(summedNode);\n",
    "    #output = aSession.run(summedNode);\n",
    "\n",
    "    # This will succeed.\n",
    "    randArrayA = np.random.rand(1, 2)\n",
    "    randArrayB = np.random.rand(1, 2)\n",
    "    output = aSession.run(summedNode, feed_dict={aNode: randArrayA, bNode: randArrayB});\n",
    "    print (\"A: \",randArrayA, \"\\nB: \", randArrayB, \"\\nA+B: \",output)\n",
    "    \n",
    "    randArrayX = np.random.rand(1024, 1024)\n",
    "    print(\"\\nX:\", randArrayX, \"\\nX*X:\", aSession.run(yNode, feed_dict={xNode: randArrayX}))  # Will succeed.\n",
    "aSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**3. Variables**: Used to integrate trainable parameters to the graph. The weights and the offset are the best examples of variables. Here is an example of variable usage in training a model:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([0.9999908], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([.30], tf.float32)\n",
    "b = tf.Variable([-.30], tf.float32)\n",
    "\n",
    "# input and output variables\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "linearModel = W*x + b\n",
    "\n",
    "# Loss (this will tell us how far we are from getting to the right answer)\n",
    "squaredDelta = tf.square(linearModel - y)\n",
    "loss = tf.reduce_sum(squaredDelta)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "'''\n",
    "An Optimizer modifies each variable according to the magnitude of the derivate of the\n",
    "loss with respect to that variable. Gradient Descent is a type of Optimizer.\n",
    "\n",
    "Gradient Descent:-\n",
    "\n",
    "Suppose there is a Hiker are on the top of a mountain in a mountain range. The Hiker is blindfolded and\n",
    "he wants to reach the ground. He starts moving in the direction that takes him lower until it starts\n",
    "going up again.\n",
    "\n",
    "Hence, the Position of hiker is weight. Length of the step is the learning rate.\n",
    "\n",
    "OR in other words:\n",
    "\n",
    "An Optimizer will calculate the change in the loss WRT to change in the variable. If the loss is \n",
    "decreasing then it will continue changing the varibale in that direction.\n",
    "'''\n",
    "\n",
    "with tf.Session() as someSession:\n",
    "    someSession.run(init)\n",
    "\n",
    "    for i in range(1000):\n",
    "        someSession.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    print (someSession.run([W, b]))\n",
    "someSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a classifier using what we have learned/used so far**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: (Binary Classificatoin) Given some training data with n features and corresponding labels (0 or 1), train a binary classifier and predict the labels on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def readData():\n",
    "    # Read the training data and separate the dependent variable from the features\n",
    "    trainDataFrame = pd.read_csv(\"train.csv\")\n",
    "    nCols = len(trainDataFrame.columns)\n",
    "    X = trainDataFrame[trainDataFrame.columns[0:nCols-1]].values\n",
    "    y = trainDataFrame[trainDataFrame.columns[nCols-1]].values\n",
    "    \n",
    "    # Encoding the dependent variable\n",
    "    # Incase of labels such as \"Yes\" and \"No\", we want to map them to a corresponding label vector\n",
    "    # that has all zeroes expect at the corresponding label.\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y = encoder.transform(y)\n",
    "    Y = one_hot_encode(y)\n",
    "    \n",
    "    return (X, Y)\n",
    "    \n",
    "def one_hot_encode(labels):\n",
    "    nLabels = len(labels)\n",
    "    nUniqueLabels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((nLabels, nUniqueLabels))\n",
    "    one_hot_encode[np.arange(nLabels), labels] = 1\n",
    "    return one_hot_encode\n",
    "    \n",
    "    \n",
    "X, Y = readData()\n",
    "X, Y = shuffle(X, Y, random_state=1)\n",
    "\n",
    "# Dividing the training data into train and dev sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.2, random_state = 415)\n",
    "\n",
    "# Inspect the shape after dividing into the right data sets\n",
    "# print ((train_x.shape))\n",
    "# print ((test_x.shape))\n",
    "# print ((train_y.shape))\n",
    "# print ((test_y.shape))\n",
    "\n",
    "# Important parameters and modelling variables\n",
    "learning_rate = 0.25\n",
    "training_epochs = 1000\n",
    "cost_history = np.empty(shape=[1], dtype=float)\n",
    "\n",
    "# dimension of the feature vector\n",
    "n_dim = X.shape[1]\n",
    "# Number of possible labels / output classes\n",
    "n_class = Y.shape[1]\n",
    "\n",
    "# Define the number of hidden layers and Neurons in each layer\n",
    "n_hidden_1 = 40\n",
    "n_hidden_2 = 40\n",
    "n_hidden_3 = 40\n",
    "n_hidden_4 = 40\n",
    "\n",
    "# input and output variables\n",
    "x = tf.placeholder(tf.float32, [None, n_dim]) # None tells that it can be any value\n",
    "W = tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "b = tf.Variable(tf.zeros([n_class]))\n",
    "y = tf.placeholder(tf.float32, [None, n_class]) # None tells that it can be any value\n",
    "\n",
    "# Defining the model\n",
    "def multiLayerPerceptron(x, weights, biases):\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights[\"h1\"], biases[\"b1\"]))\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights[\"h2\"], biases[\"b2\"]))\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights[\"h3\"], biases[\"b3\"]))\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights[\"h4\"], biases[\"b4\"]))\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    \n",
    "    layer_out = tf.matmul(layer_4, weights[\"out\"], biases[\"out\"])\n",
    "    \n",
    "    return layer_out\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "    \"h4\": tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "    \"out\": tf.Variable(tf.truncated_normal([n_hidden_4, n_class]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "    \"b4\": tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "    \"out\": tf.Variable(tf.truncated_normal([n_class]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
