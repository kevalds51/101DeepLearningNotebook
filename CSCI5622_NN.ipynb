{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About TensorFlow and Deep Learning:-\n",
    "\n",
    "**TensorFlow:** A Python library that is used for modelling/implementation of deep learning models/networks. TensorFlow = Tensors + Flow. Tensor corresponds to the way data is represented in this library and Flow depicts the flow of these Tensors through the <i>computation graph</i>.\n",
    "\n",
    "A <i>computation graph</i> is series of TensorFlow operations arranged into a graph of nodes.\n",
    "\n",
    "**Tensors**: The standard way of representing data in TensorFlow. Tensors are nothing but multidimensional arrays, an extension of matrices (2D tables) to data with higher dimensions.\n",
    "\n",
    "Note: Only tensors may be passed between nodes in a computation graph.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li> Dimensionality is measured as **Ranks**:-\n",
    "\n",
    "<ol>\n",
    "    \n",
    "   <li> Rank 0 -> a Scalar, example: s = 482\n",
    "\n",
    "   <li> Rank 1 -> a Vector, example: v = [1, 2, 3]\n",
    "\n",
    "   <li> Rank 2 -> a Matrix, example: m = [[1,5,6], [2,5,6], [3,5,6]]\n",
    "\n",
    "   <li> Rank 3 -> a 3-Tesor or a cube holding some data, example: t = [[[1,5,6], [2,5,6], [3,5,6]], [[1,5,6], [2,5,6], [3,5,6]], [[1,5,6], [2,5,6], [3,5,6]]]\n",
    "    \n",
    "</ol>\n",
    "\n",
    "    ...\n",
    "\n",
    "   ** Rank n -> n-Tensor **\n",
    "\n",
    "<li> Tensor Data Types:- \n",
    "\n",
    "Tensorflow automattically assigns the correct data type. If you want to specifically assign the data type in order to save memory or do some other operation, it is possible.\n",
    "\n",
    "<ol>\n",
    "  \n",
    "   <li> DT_FLOAT -> tf.float32\n",
    "    \n",
    "   <li> DT_DOUBLE-> tf.float64\n",
    "    \n",
    "   <li> DT_INT8 -> tf.int8\n",
    "\n",
    "   <li> DT_UINT8 -> tf.uint8\n",
    "    \n",
    "   ...\n",
    "    \n",
    "   <li> DT_INT64 -> tf.int64\n",
    "    \n",
    "   <li> DT_STRING -> tf.string\n",
    "    \n",
    "   <li> DT_BOOL -> tf.bool\n",
    "    \n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Coding structure**\n",
    "\n",
    "It consists of two sections in particular:-\n",
    "\n",
    "<ul>\n",
    "    <li> Building a computation graph.\n",
    "    <li> Running the computation graph.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** - Building a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32) Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Import the TensorFlow library\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Defining two constant nodes\n",
    "aNode = tf.constant(5.0)\n",
    "bNode = tf.constant(1.9, tf.float32)\n",
    "\n",
    "cNode = aNode*bNode;\n",
    "\n",
    "# At this point, they are just abstract Tensors and not actual calculations are running.\n",
    "# Only operations are created.\n",
    "print (aNode, bNode, cNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** - Running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 1.9]\n",
      "9.5\n"
     ]
    }
   ],
   "source": [
    "# To execute the graph, we run it inside a session\n",
    "# A session places graph operations onto devices such as CPU/GPU.\n",
    "\n",
    "''' Method 1 '''\n",
    "\n",
    "aSession = tf.Session()\n",
    "\n",
    "print (aSession.run([aNode, bNode]))\n",
    "# You need to close a session in order to free up the resources it used.\n",
    "aSession.close()\n",
    "\n",
    "''' Method 2 '''\n",
    "\n",
    "with tf.Session() as bSession:\n",
    "    # You just need to run the output Tensor\n",
    "    output = bSession.run(cNode)\n",
    "    print (output)\n",
    "bSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing TensorFlow: TensorBoard**\n",
    "\n",
    "TensorBoard is a suite of web-applications for understanding your Tensor graphs.\n",
    "\n",
    "The most convinient way to do this is using FileWriter by:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aNode = tf.constant(5.0)\n",
    "bNode = tf.constant(1.9, tf.float32)\n",
    "cNode = aNode*bNode;\n",
    "\n",
    "with tf.Session() as bSession:\n",
    "    output = bSession.run(cNode)    \n",
    "    # Give the first argument as a path and the second as the graph of the session\n",
    "    aFileWriter = tf.summary.FileWriter('aSimpleGraph', aSession.graph)\n",
    "bSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the log, go to the directory in your command line and type:\n",
    "\n",
    "   ** tensorboad --logdir=\"TensorFlow\" **\n",
    "   \n",
    "It will say that the TensorBoard runs at http://localhost:6006/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been seeing constant nodes in the previous examples. Let us dig deeper into them and other nodes in TensorFlow\n",
    "\n",
    "**1. Constants**: As the name suggests, they have hardcoded values and take in no inputs. All they do is output this internal value in an active session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Placeholders**: These nodes are able to take in external input, assuming that they will be provided one at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  [[0.23313773 0.2844311 ]] \n",
      "B:  [[0.10211448 0.8355557 ]] \n",
      "A+B:  [[0.33525223 1.1199868 ]]\n",
      "\n",
      "X: [[0.76313563 0.9065525  0.32440845 ... 0.49741626 0.50019583 0.11704756]\n",
      " [0.37421243 0.38296898 0.25924542 ... 0.86231968 0.40420137 0.71002387]\n",
      " [0.13893258 0.46083097 0.82178146 ... 0.86745065 0.29185133 0.53283617]\n",
      " ...\n",
      " [0.27915509 0.69321072 0.56813464 ... 0.16981308 0.9930976  0.0525749 ]\n",
      " [0.34943331 0.44936957 0.39633732 ... 0.09998226 0.67308295 0.97709263]\n",
      " [0.0202177  0.40685868 0.4393103  ... 0.13153815 0.48851592 0.57090187]] \n",
      "X*X: [[254.31223 261.9021  259.7486  ... 262.18668 258.52127 261.90085]\n",
      " [250.84096 260.94064 259.23694 ... 263.15625 258.04282 264.0538 ]\n",
      " [255.56479 265.75198 265.67847 ... 267.9383  256.8534  264.97784]\n",
      " ...\n",
      " [246.74905 255.22658 251.13799 ... 253.10086 250.78667 254.35167]\n",
      " [252.78789 258.29706 255.91455 ... 265.49756 259.21896 263.75568]\n",
      " [246.37346 258.87375 258.57007 ... 260.93933 256.1042  256.2373 ]]\n"
     ]
    }
   ],
   "source": [
    "aNode = tf.placeholder(tf.float32)\n",
    "bNode = tf.placeholder(tf.float32)\n",
    "\n",
    "summedNode = aNode + bNode;\n",
    "# You can also use the in-build TensorFlow operations\n",
    "# summedNode = tf.add(aNode, bNode);\n",
    "\n",
    "xNode = tf.placeholder(tf.float32, shape=(1024, 1024))\n",
    "# matmul is the in-built TensorFlow operation for matrix-multiplication\n",
    "yNode = tf.matmul(xNode, xNode)\n",
    "\n",
    "with tf.Session() as aSession:\n",
    "    # ERROR: will fail because placeholder was not fed.\n",
    "    #output = aSession.run(summedNode);\n",
    "    #output = aSession.run(summedNode);\n",
    "\n",
    "    # This will succeed.\n",
    "    randArrayA = np.random.rand(1, 2)\n",
    "    randArrayB = np.random.rand(1, 2)\n",
    "    output = aSession.run(summedNode, feed_dict={aNode: randArrayA, bNode: randArrayB});\n",
    "    print (\"A: \",randArrayA, \"\\nB: \", randArrayB, \"\\nA+B: \",output)\n",
    "    \n",
    "    randArrayX = np.random.rand(1024, 1024)\n",
    "    print(\"\\nX:\", randArrayX, \"\\nX*X:\", aSession.run(yNode, feed_dict={xNode: randArrayX}))  # Will succeed.\n",
    "aSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**3. Variables**: Used to integrate trainable parameters to the graph. The weights and the offset are the best examples of variables. Here is an example of variable usage in training a model:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([0.9999908], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([.30], tf.float32)\n",
    "b = tf.Variable([-.30], tf.float32)\n",
    "\n",
    "# input and output variables\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "linearModel = W*x + b\n",
    "\n",
    "# Loss (this will tell us how far we are from getting to the right answer)\n",
    "squaredDelta = tf.square(linearModel - y)\n",
    "loss = tf.reduce_sum(squaredDelta)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "'''\n",
    "An Optimizer modifies each variable according to the magnitude of the derivate of the\n",
    "loss with respect to that variable. Gradient Descent is a type of Optimizer.\n",
    "\n",
    "Gradient Descent:-\n",
    "\n",
    "Suppose there is a Hiker are on the top of a mountain in a mountain range. The Hiker is blindfolded and\n",
    "he wants to reach the ground. He starts moving in the direction that takes him lower until it starts\n",
    "going up again.\n",
    "\n",
    "Hence, the Position of hiker is weight. Length of the step is the learning rate.\n",
    "\n",
    "OR in other words:\n",
    "\n",
    "An Optimizer will calculate the change in the loss WRT to change in the variable. If the loss is \n",
    "decreasing then it will continue changing the varibale in that direction.\n",
    "'''\n",
    "\n",
    "with tf.Session() as someSession:\n",
    "    someSession.run(init)\n",
    "\n",
    "    for i in range(1000):\n",
    "        someSession.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    print (someSession.run([W, b]))\n",
    "someSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a classifier using what we have learned/used so far**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: (Binary Classificatoin) Given some training data with n features and corresponding labels (0 or 1), train a binary classifier and predict the labels on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  60\n",
      "Number of labels:  2\n",
      "\n",
      "epoch:  0  Acc:  0.54545456\n",
      "\n",
      "epoch:  1  Acc:  0.54545456\n",
      "\n",
      "epoch:  2  Acc:  0.58181816\n",
      "\n",
      "epoch:  3  Acc:  0.56969696\n",
      "\n",
      "epoch:  4  Acc:  0.58787876\n",
      "\n",
      "epoch:  5  Acc:  0.6121212\n",
      "\n",
      "epoch:  6  Acc:  0.6121212\n",
      "\n",
      "epoch:  7  Acc:  0.6363636\n",
      "\n",
      "epoch:  8  Acc:  0.6363636\n",
      "\n",
      "epoch:  9  Acc:  0.6484848\n",
      "\n",
      "epoch:  10  Acc:  0.6606061\n",
      "\n",
      "epoch:  11  Acc:  0.6848485\n",
      "\n",
      "epoch:  12  Acc:  0.6484848\n",
      "\n",
      "epoch:  13  Acc:  0.6060606\n",
      "\n",
      "epoch:  14  Acc:  0.6242424\n",
      "\n",
      "epoch:  15  Acc:  0.6121212\n",
      "\n",
      "epoch:  16  Acc:  0.6363636\n",
      "\n",
      "epoch:  17  Acc:  0.53939396\n",
      "\n",
      "epoch:  18  Acc:  0.56969696\n",
      "\n",
      "epoch:  19  Acc:  0.53939396\n",
      "\n",
      "epoch:  20  Acc:  0.6\n",
      "\n",
      "epoch:  21  Acc:  0.58787876\n",
      "\n",
      "epoch:  22  Acc:  0.6363636\n",
      "\n",
      "epoch:  23  Acc:  0.58787876\n",
      "\n",
      "epoch:  24  Acc:  0.6484848\n",
      "\n",
      "epoch:  25  Acc:  0.58787876\n",
      "\n",
      "epoch:  26  Acc:  0.5939394\n",
      "\n",
      "epoch:  27  Acc:  0.6\n",
      "\n",
      "epoch:  28  Acc:  0.58181816\n",
      "\n",
      "epoch:  29  Acc:  0.6\n",
      "\n",
      "epoch:  30  Acc:  0.58787876\n",
      "\n",
      "epoch:  31  Acc:  0.58181816\n",
      "\n",
      "epoch:  32  Acc:  0.58181816\n",
      "\n",
      "epoch:  33  Acc:  0.58181816\n",
      "\n",
      "epoch:  34  Acc:  0.6121212\n",
      "\n",
      "epoch:  35  Acc:  0.58181816\n",
      "\n",
      "epoch:  36  Acc:  0.6060606\n",
      "\n",
      "epoch:  37  Acc:  0.6181818\n",
      "\n",
      "epoch:  38  Acc:  0.5939394\n",
      "\n",
      "epoch:  39  Acc:  0.6242424\n",
      "\n",
      "epoch:  40  Acc:  0.6060606\n",
      "\n",
      "epoch:  41  Acc:  0.6242424\n",
      "\n",
      "epoch:  42  Acc:  0.6121212\n",
      "\n",
      "epoch:  43  Acc:  0.6424242\n",
      "\n",
      "epoch:  44  Acc:  0.6060606\n",
      "\n",
      "epoch:  45  Acc:  0.6181818\n",
      "\n",
      "epoch:  46  Acc:  0.58181816\n",
      "\n",
      "epoch:  47  Acc:  0.6121212\n",
      "\n",
      "epoch:  48  Acc:  0.630303\n",
      "\n",
      "epoch:  49  Acc:  0.6363636\n",
      "\n",
      "epoch:  50  Acc:  0.58181816\n",
      "\n",
      "epoch:  51  Acc:  0.6242424\n",
      "\n",
      "epoch:  52  Acc:  0.6181818\n",
      "\n",
      "epoch:  53  Acc:  0.6484848\n",
      "\n",
      "epoch:  54  Acc:  0.5939394\n",
      "\n",
      "epoch:  55  Acc:  0.6060606\n",
      "\n",
      "epoch:  56  Acc:  0.6606061\n",
      "\n",
      "epoch:  57  Acc:  0.72727275\n",
      "\n",
      "epoch:  58  Acc:  0.6545454\n",
      "\n",
      "epoch:  59  Acc:  0.58787876\n",
      "\n",
      "epoch:  60  Acc:  0.6121212\n",
      "\n",
      "epoch:  61  Acc:  0.74545455\n",
      "\n",
      "epoch:  62  Acc:  0.6606061\n",
      "\n",
      "epoch:  63  Acc:  0.58787876\n",
      "\n",
      "epoch:  64  Acc:  0.6484848\n",
      "\n",
      "epoch:  65  Acc:  0.72727275\n",
      "\n",
      "epoch:  66  Acc:  0.75151515\n",
      "\n",
      "epoch:  67  Acc:  0.6121212\n",
      "\n",
      "epoch:  68  Acc:  0.56363636\n",
      "\n",
      "epoch:  69  Acc:  0.75757575\n",
      "\n",
      "epoch:  70  Acc:  0.76363635\n",
      "\n",
      "epoch:  71  Acc:  0.6\n",
      "\n",
      "epoch:  72  Acc:  0.58181816\n",
      "\n",
      "epoch:  73  Acc:  0.75757575\n",
      "\n",
      "epoch:  74  Acc:  0.7818182\n",
      "\n",
      "epoch:  75  Acc:  0.6484848\n",
      "\n",
      "epoch:  76  Acc:  0.58787876\n",
      "\n",
      "epoch:  77  Acc:  0.6606061\n",
      "\n",
      "epoch:  78  Acc:  0.76363635\n",
      "\n",
      "epoch:  79  Acc:  0.7939394\n",
      "\n",
      "epoch:  80  Acc:  0.6363636\n",
      "\n",
      "epoch:  81  Acc:  0.5939394\n",
      "\n",
      "epoch:  82  Acc:  0.6969697\n",
      "\n",
      "epoch:  83  Acc:  0.8121212\n",
      "\n",
      "epoch:  84  Acc:  0.8\n",
      "\n",
      "epoch:  85  Acc:  0.6121212\n",
      "\n",
      "epoch:  86  Acc:  0.6181818\n",
      "\n",
      "epoch:  87  Acc:  0.74545455\n",
      "\n",
      "epoch:  88  Acc:  0.8121212\n",
      "\n",
      "epoch:  89  Acc:  0.72121215\n",
      "\n",
      "epoch:  90  Acc:  0.5939394\n",
      "\n",
      "epoch:  91  Acc:  0.7151515\n",
      "\n",
      "epoch:  92  Acc:  0.8181818\n",
      "\n",
      "epoch:  93  Acc:  0.8060606\n",
      "\n",
      "epoch:  94  Acc:  0.6606061\n",
      "\n",
      "epoch:  95  Acc:  0.6181818\n",
      "\n",
      "epoch:  96  Acc:  0.76363635\n",
      "\n",
      "epoch:  97  Acc:  0.7878788\n",
      "\n",
      "epoch:  98  Acc:  0.6787879\n",
      "\n",
      "epoch:  99  Acc:  0.6363636\n",
      "[26.490443603997456, 0.6798413815269296, 0.6409204045878736, 0.7334201139575249, 0.7807535769798174, 0.8176727542711805, 0.8716892111554868, 0.8934379431327476, 0.959859445808647, 1.0196084644838603, 1.063623784320121, 1.1350203338683693, 1.1912309502991647, 1.2700138722017584, 1.3974947217048872, 1.3920549311255053, 1.4889996954211135, 1.5302629932368248, 1.6426347192419324, 1.5339942714262338, 1.477565702135338, 1.4948909163043478, 1.5862523581702574, 1.5128054319372186, 1.505194159397518, 1.464400903441793, 1.4471284904600612, 1.4862215157747547, 1.4839255339025752, 1.4757847703371572, 1.5516753259931828, 1.6372402195080258, 1.7270532000661583, 1.6944561871111277, 1.7989480703788931, 1.7801283251138382, 1.7285881254532522, 1.8361187548496005, 1.9175400935131284, 1.9804002696190142, 1.9581240306288177, 2.0846540256460044, 1.951375015704803, 2.0713490090834603, 2.106708843677415, 2.268712783731383, 2.1891714572461356, 2.171432025430621, 2.1536365275030454, 2.337104881511007, 2.414292925601297, 2.489661283323205, 2.3469463360251117, 2.5167025749707683, 2.5149593472290825, 2.728645760449201, 2.3454703794538405, 2.371967031085961, 2.5888666927909147, 2.9954576094382697, 2.4454300677075023, 2.513695153371041, 2.6517362902804873, 3.0618916962535008, 2.558861912238424, 2.4119342448405314, 2.7164148686604666, 3.263238551104816, 2.9522607698291794, 2.390616402288534, 2.7067908852144673, 3.243310634636295, 3.052935779485235, 2.5158215531121937, 2.82337108739024, 3.2414815836432123, 3.5004611544185167, 2.7353827660728536, 2.51878308712646, 2.9214273969256928, 3.4728329933230278, 3.4740348956837437, 2.7628158535951064, 3.0633068614740866, 3.1464965917664567, 3.792826598994541, 3.417429475419992, 2.849135537406225, 3.377999260341896, 3.555617822427602, 4.010458454279164, 3.040314812353494, 3.0646241716765, 3.566373293865002, 4.110014533371144, 3.9675011733909877, 3.263692913137742, 3.483849326472427, 3.911114420441325, 3.9475047481896826]\n",
      "[0.54545456, 0.54545456, 0.58181816, 0.56969696, 0.58787876, 0.6121212, 0.6121212, 0.6363636, 0.6363636, 0.6484848, 0.6606061, 0.6848485, 0.6484848, 0.6060606, 0.6242424, 0.6121212, 0.6363636, 0.53939396, 0.56969696, 0.53939396, 0.6, 0.58787876, 0.6363636, 0.58787876, 0.6484848, 0.58787876, 0.5939394, 0.6, 0.58181816, 0.6, 0.58787876, 0.58181816, 0.58181816, 0.58181816, 0.6121212, 0.58181816, 0.6060606, 0.6181818, 0.5939394, 0.6242424, 0.6060606, 0.6242424, 0.6121212, 0.6424242, 0.6060606, 0.6181818, 0.58181816, 0.6121212, 0.630303, 0.6363636, 0.58181816, 0.6242424, 0.6181818, 0.6484848, 0.5939394, 0.6060606, 0.6606061, 0.72727275, 0.6545454, 0.58787876, 0.6121212, 0.74545455, 0.6606061, 0.58787876, 0.6484848, 0.72727275, 0.75151515, 0.6121212, 0.56363636, 0.75757575, 0.76363635, 0.6, 0.58181816, 0.75757575, 0.7818182, 0.6484848, 0.58787876, 0.6606061, 0.76363635, 0.7939394, 0.6363636, 0.5939394, 0.6969697, 0.8121212, 0.8, 0.6121212, 0.6181818, 0.74545455, 0.8121212, 0.72121215, 0.5939394, 0.7151515, 0.8181818, 0.8060606, 0.6606061, 0.6181818, 0.76363635, 0.7878788, 0.6787879, 0.6363636]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def readData():\n",
    "    # Read the training data and separate the dependent variable from the features\n",
    "    trainDataFrame = pd.read_csv(\"sonar.csv\")\n",
    "    nCols = len(trainDataFrame.columns)\n",
    "    X = trainDataFrame[trainDataFrame.columns[0:nCols-1]].values\n",
    "    y = trainDataFrame[trainDataFrame.columns[nCols-1]].values\n",
    "    \n",
    "    # Encoding the dependent variable\n",
    "    # Incase of labels such as \"Yes\" and \"No\", we want to map them to a corresponding label vector\n",
    "    # that has all zeroes expect at the corresponding label.\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y = encoder.transform(y)\n",
    "    Y = one_hot_encode(y)\n",
    "    \n",
    "    return (X, Y)\n",
    "    \n",
    "def one_hot_encode(labels):\n",
    "    nLabels = len(labels)\n",
    "    nUniqueLabels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((nLabels, nUniqueLabels))\n",
    "    one_hot_encode[np.arange(nLabels), labels] = 1\n",
    "    return one_hot_encode\n",
    "    \n",
    "    \n",
    "X, Y = readData()\n",
    "X, Y = shuffle(X, Y, random_state=1)\n",
    "\n",
    "# print (Y)\n",
    "\n",
    "# Dividing the training data into train and dev sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.2, )#random_state = 415)\n",
    "\n",
    "# print (train_y)\n",
    "\n",
    "# Inspect the shape after dividing into the right data sets\n",
    "# print ((train_x.shape))\n",
    "# print ((test_x.shape))\n",
    "# print ((train_y.shape))\n",
    "# print ((test_y.shape))\n",
    "\n",
    "# Important parameters and modelling variables\n",
    "learning_rate = 0.25\n",
    "training_epochs = 100\n",
    "cost_history = np.empty(shape=[1], dtype=float)\n",
    "\n",
    "# dimension of the feature vector\n",
    "n_dim = X.shape[1]\n",
    "print (\"Number of features: \", n_dim)\n",
    "# Number of possible labels / output classes\n",
    "n_class = Y.shape[1]\n",
    "print (\"Number of labels: \", n_class)\n",
    "\n",
    "# Define the number of hidden layers and Neurons in each layer\n",
    "n_hidden_1 = 40\n",
    "n_hidden_2 = 30\n",
    "n_hidden_3 = 40\n",
    "n_hidden_4 = 40\n",
    "\n",
    "# input and output variables\n",
    "x = tf.placeholder(tf.float32, [None, n_dim]) # 'None' tells that it can be any value\n",
    "W = tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "b = tf.Variable(tf.zeros([n_class]))\n",
    "y_ = tf.placeholder(tf.float32, [None, n_class]) # 'None' tells that it can be any value\n",
    "\n",
    "# Defining the model\n",
    "def multiLayerPerceptron(x, weights, biases):\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights[\"h1\"]), biases[\"b1\"])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights[\"h2\"]), biases[\"b2\"])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights[\"h3\"]), biases[\"b3\"])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights[\"h4\"]), biases[\"b4\"])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    \n",
    "    layer_out = tf.matmul(layer_4, weights[\"out\"]) + biases[\"out\"]\n",
    "    \n",
    "    return layer_out\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "    \"h4\": tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "    \"out\": tf.Variable(tf.truncated_normal([n_hidden_4, n_class]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "    \"b4\": tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "    \"out\": tf.Variable(tf.truncated_normal([n_class]))\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "y = multiLayerPerceptron(x, weights, biases)\n",
    "\n",
    "costFunction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))\n",
    "trainingStep = tf.train.GradientDescentOptimizer(learning_rate).minimize(costFunction)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "mseHistory = []\n",
    "accHistory = []\n",
    "\n",
    "someSession = tf.Session()\n",
    "someSession.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    someSession.run(trainingStep, feed_dict={x: train_x, y_: train_y})\n",
    "    cost = someSession.run(trainingStep, {x: train_x, y_: train_y})\n",
    "    cost_history = np.append(cost_history, cost)\n",
    "    correctPred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    pred_y = someSession.run(y, {x: test_x})\n",
    "    mse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
    "    mse_ = someSession.run(mse)\n",
    "    mseHistory.append(mse_)\n",
    "    acc = (someSession.run(acc, {x: train_x, y_: train_y}))\n",
    "    accHistory.append(acc)\n",
    "\n",
    "    print (\"\\nepoch: \", epoch, \" Acc: \", acc)\n",
    "        \n",
    "# save_path = saver.save(someSession, )\n",
    "someSession.close()\n",
    "\n",
    "print (mseHistory)\n",
    "print (accHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
